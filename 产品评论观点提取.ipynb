{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "产品评论观点提取.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiN9I0TWHYcS1NKnqhNA+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxianggang1/machinelearning/blob/main/%E4%BA%A7%E5%93%81%E8%AF%84%E8%AE%BA%E8%A7%82%E7%82%B9%E6%8F%90%E5%8F%96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t5jKCczaPJeL"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "from functools import partial\n",
        "import inspect\n",
        "import numpy as np\n",
        "import collections\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "# 解压缩\n",
        "import zipfile\n",
        "zipname = '产品评论观点提取-new.zip'\n",
        "extractpath = './data'\n",
        "#注意压缩格式选择\n",
        "frzip = zipfile.ZipFile(zipname, 'r', zipfile.ZIP_DEFLATED)\n",
        "#将所有文件加压缩到指定目录\n",
        "frzip.extractall(extractpath)\n",
        "frzip.close()\n",
        "\n",
        "file_name='data/train_data_public.csv'\n",
        "train_data = pd.read_csv('data/train_data_public.csv')\n",
        "test_data = pd.read_csv('data/test_public.csv')\n",
        "\n",
        "train_data['BIO_anno'] = train_data['BIO_anno'].apply(lambda x:x.split(' '))\n",
        "train_data['training_data'] = train_data.apply(lambda row: (list(row['text']), row['BIO_anno']), axis = 1)\n",
        "test_data['testing_data'] = test_data.apply(lambda row: (list(row['text'])), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_txt = []\n",
        "testing_data_txt = []\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    training_data_txt.append(train_data.iloc[i]['training_data'])\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "    testing_data_txt.append(test_data.iloc[i]['testing_data'])"
      ],
      "metadata": {
        "id": "jTR1a2PvQO5u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    # 返回vec的dim为1维度上的最大值索引\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    # 将句子转化为ID\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "# 前向算法是不断累积之前的结果，这样就会有个缺点\n",
        "# 指数和累积到一定程度后，会超过计算机浮点值的最大值，变成inf，这样取log后也是inf\n",
        "# 为了避免这种情况，用一个合适的值clip去提指数和的公因子，这样就不会使某项变得过大而无法计算\n",
        "# SUM = log(exp(s1)+exp(s2)+...+exp(s100))\n",
        "#     = log{exp(clip)*[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]}\n",
        "#     = clip + log[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]\n",
        "# where clip=max\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim    # word embedding dim\n",
        "        self.hidden_dim = hidden_dim          # Bi-LSTM hidden dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # 将BiLSTM提取的特征向量映射到特征空间，即经过全连接得到发射分数\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # 转移矩阵的参数初始化，transitions[i,j]代表的是从第j个tag转移到第i个tag的转移分数\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # 初始化所有其他tag转移到START_TAG的分数非常小，即不可能由其他tag转移到START_TAG\n",
        "        # 初始化STOP_TAG转移到所有其他tag的分数非常小，即不可能由STOP_TAG转移到其他tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # 初始化LSTM的参数\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        # 通过Bi-LSTM提取特征\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # 计算给定tag序列的分数，即一条路径的分数\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            # 递推计算路径分数：转移分数 + 发射分数\n",
        "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # 通过前向算法递推计算\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # 初始化step 0即START位置的发射分数，START_TAG取0其他位置取-10000\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # 将初始化START位置为0的发射分数赋值给previous\n",
        "        previous = init_alphas\n",
        "        # 迭代整个句子\n",
        "        for obs in feats:\n",
        "            # 当前时间步的前向tensor\n",
        "            alphas_t = []\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # 取出当前tag的发射分数，与之前时间步的tag无关\n",
        "                emit_score = obs[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
        "                # 取出当前tag由之前tag转移过来的转移分数\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # 当前路径的分数：之前时间步分数 + 转移分数 + 发射分数\n",
        "                next_tag_var = previous + trans_score + emit_score\n",
        "                # 对当前分数取log-sum-exp\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            # 更新previous 递推计算下一个时间步\n",
        "            previous = torch.cat(alphas_t).view(1, -1)\n",
        "        # 考虑最终转移到STOP_TAG\n",
        "        terminal_var = previous + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        # 计算最终的分数\n",
        "        scores = log_sum_exp(terminal_var)\n",
        "        return scores\n",
        "\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # 初始化viterbi的previous变量\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        previous = init_vvars\n",
        "        for obs in feats:\n",
        "            # 保存当前时间步的回溯指针\n",
        "            bptrs_t = []\n",
        "            # 保存当前时间步的viterbi变量\n",
        "            viterbivars_t = []\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # 维特比算法记录最优路径时只考虑上一步的分数以及上一步tag转移到当前tag的转移分数\n",
        "                # 并不取决与当前tag的发射分数\n",
        "                next_tag_var = previous + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # 更新previous，加上当前tag的发射分数obs\n",
        "            previous = (torch.cat(viterbivars_t) + obs).view(1, -1)\n",
        "            # 回溯指针记录当前时间步各个tag来源前一步的tag\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        # 考虑转移到STOP_TAG的转移分数\n",
        "        terminal_var = previous + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "        # 通过回溯指针解码出最优路径\n",
        "        best_path = [best_tag_id]\n",
        "        # best_tag_id作为线头，反向遍历backpointers找到最优路径\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # 去除START_TAG\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        # CRF损失函数由两部分组成，真实路径的分数和所有路径的总分数。\n",
        "        # 真实路径的分数应该是所有路径中分数最高的。\n",
        "        # log真实路径的分数/log所有可能路径的分数，越大越好，构造crf loss函数取反，loss越小越好\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "    def forward(self, sentence):\n",
        "        # 通过BiLSTM提取发射分数\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # 根据发射分数以及转移分数，通过viterbi解码找到一条最优路径\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "metadata": {
        "id": "nc6WhMdpQTif"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 11\n",
        "HIDDEN_DIM = 6\n",
        "import time\n",
        "t = time.time()\n",
        "\n",
        "# 将训练集汉字使用数字表示\n",
        "# 为了方便调试，先使用100条数据进行模型训练，选手可以采用全量数据进行训练\n",
        "training_data = training_data_txt\n",
        "word_to_ix = {}\n",
        "for sentence, tags in training_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "# 将测试集汉字使用数字表示\n",
        "testing_data = testing_data_txt\n",
        "for sentence in testing_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "tag_to_ix = { \"O\": 0, \"B-BANK\": 1, \"I-BANK\": 2, \"B-PRODUCT\":3,'I-PRODUCT':4,\n",
        "             'B-COMMENTS_N':5, 'I-COMMENTS_N':6, 'B-COMMENTS_ADJ':7,\n",
        "             'I-COMMENTS_ADJ':8, START_TAG: 9, STOP_TAG: 10}\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# 训练前检查模型预测结果\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
        "    print(model(precheck_sent))\n",
        "    a = model(precheck_sent)\n",
        "    a = pd.Series(a)\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phkaEdhLQYv-",
        "outputId": "9be22b62-51a2-4f13-fc89-f9495430ebfe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor(125.1090), [6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 5, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 6, 0, 7, 4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    print('the',epoch,' epoch')\n",
        "    print(f'Time Taken: {round(time.time()-t)} seconds')\n",
        "    for sentence, tags in training_data:\n",
        "        # 第一步，pytorch梯度累积，需要清零梯度\n",
        "        model.zero_grad()\n",
        "        # 第二步，将输入转化为tensors\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
        "\n",
        "        # 进行前向计算，取出crf loss\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # 第四步，计算loss，梯度，通过optimier更新参数\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fptWql6eQf4o",
        "outputId": "66c6bea2-8ff5-4528-f018-ed0326a6b7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 0  epoch\n",
            "Time Taken: 19 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练结束查看模型预测结果，对比观察模型是否学到\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[3][0], word_to_ix)\n",
        "    print(model(precheck_sent))\n",
        "    a = model(precheck_sent)\n",
        "    a = pd.Series(a)\n",
        "    a.to_csv('test1.csv')\n",
        "# We got it!"
      ],
      "metadata": {
        "id": "vrg71ptGQi8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#情感分类\n",
        "X_train = list(train_data['text'])\n",
        "X_test = list(train_data['text'][800:1000])\n",
        "\n",
        "y_train = list(train_data['class'])\n",
        "y_test = list(train_data['class'][800:1000])\n",
        "\n",
        "test_data_sent = list(test_data['text']) # 列表格式\n",
        "text_all = X_train + test_data_sent\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
        "vectoriser.fit(text_all)\n",
        "print(f'Vectoriser fitted.')\n",
        "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
        "X_train = vectoriser.transform(X_train)\n",
        "test_data_text  = vectoriser.transform(test_data_sent)\n",
        "# X_test  = vectoriser.transform(X_test)\n",
        "print(f'Data Transformed.')"
      ],
      "metadata": {
        "id": "1nVcy8NJQl9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
        "LRmodel.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
        "LRmodel.fit(X_train, y_train)\n",
        "\n",
        "import pickle\n",
        "file = open('vectoriser-ngram-(1,2).pickle','wb')\n",
        "pickle.dump(vectoriser, file)\n",
        "file.close()\n",
        "\n",
        "file = open('Sentiment-LR.pickle','wb')\n",
        "pickle.dump(LRmodel, file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "lr8b3XNUQpPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "    '''\n",
        "    Replace '..path/' by the path of the saved models.\n",
        "    '''\n",
        "\n",
        "    # Load the vectoriser.\n",
        "    file = open('./vectoriser-ngram-(1,2).pickle', 'rb')\n",
        "    vectoriser = pickle.load(file)\n",
        "    file.close()\n",
        "    # Load the LR Model.\n",
        "    file = open('./Sentiment-LR.pickle', 'rb')\n",
        "    LRmodel = pickle.load(file)\n",
        "    file.close()\n",
        "\n",
        "    return vectoriser, LRmodel\n",
        "\n",
        "\n",
        "def predict(vectoriser, model, text):\n",
        "    # Predict the sentiment\n",
        "    textdata = vectoriser.transform(text)\n",
        "    sentiment = model.predict(textdata)\n",
        "\n",
        "    # Make a list of text with sentiment.\n",
        "    data = []\n",
        "    for text, pred in zip(text, sentiment):\n",
        "        data.append((text, pred))\n",
        "\n",
        "    # Convert the list into a Pandas DataFrame.\n",
        "    df = pd.DataFrame(data, columns=['text', 'class'])\n",
        "    return df\n",
        "\n",
        "\n",
        "sentiment_result = predict(vectoriser, LRmodel, X_test)\n",
        "\n",
        "test_data = pd.read_csv('./test_public.csv')\n",
        "ix_to_tag = dict([v,k] for k, v in tag_to_ix.items())"
      ],
      "metadata": {
        "id": "9CJNI_5wQsJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实体识别结果\n",
        "result = []\n",
        "with torch.no_grad():\n",
        "    for i in range(len(test_data)):\n",
        "        precheck_sent = prepare_sequence(test_data.iloc[i][1], word_to_ix)\n",
        "        sig_res = model(precheck_sent)[1]\n",
        "        for i in range(len(sig_res)):\n",
        "            sig_res[i] = ix_to_tag[sig_res[i]]\n",
        "        result.append(' '.join(sig_res))\n",
        "test_data['BIO_anno'] = result\n",
        "\n",
        "# 预测test data 的sentiment 分类\n",
        "sentiment_result = predict(vectoriser, LRmodel, test_data_sent)\n",
        "test_data['class'] = list(sentiment_result['class'])\n",
        "# test_data.to_csv('test_baseline.csv', index = None)\n"
      ],
      "metadata": {
        "id": "Wv13Ox5GQvnP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}